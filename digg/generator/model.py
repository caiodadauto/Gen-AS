"""Pytorch implementation of models that compose GraphRNN.

Original implementation can be found [here](https://github.com/JiaxuanYou/graph-generation).

.. _GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Model (2018)
   https://arxiv.org/abs/1802.08773

"""

from typing import Optional

import torch
import torch.nn.init as init
from torch import nn, Tensor
from torch.autograd import Variable
from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence


class PlainGRU(nn.Module):
    """Define a GRU variation to work with optional input and output transformations.

    Attributes:
        embedding_size: The size of the space which is used to map the
            input and output tensors.
        hidden_size: The size of the latent space.
        num_layers: Number of layers used for GRU, and for input and output mapping.
        device: The device to be used, i.e., `cpu` or `gpu`.
        has_input: Define if the input transformation is required or not.
        has_output: Define if the output transformation is required or not.
        input: The linear transformation applied to the input, if required.
        rnn: The GRU instantiation.
        output: The linear transformation applied to the output, if required.
        hidden: The hidden state of the GRU instantiation. It should be defined
            before the forward calculation.
        relu: The ReLU activation function.

    """

    def __init__(
        self,
        input_size: int,
        embedding_size: int,
        hidden_size: int,
        num_layers: int,
        device: str,
        has_input: bool = True,
        has_output: bool = False,
        output_size: Optional[int] = None,
    ):
        """Initializes `PlainGRU`.

        Args:
            input_size: The dimension of the input tensors.
            embedding_size: The dimension of the space which is used to map the
                input and output tensors.
            hidden_size: The dimension of the latent space.
            num_layers: Number of layers used for GRU, and for input and output mapping.
            device: The device to be used, i.e., `cpu` or `gpu`.
            has_input: Define if the input transformation is required or not.
            has_output: Define if the output transformation is required or not.
            output_size: The dimension of the output tensors,
                which is required only if `has_output=True`.

        """
        super(PlainGRU, self).__init__()
        self.num_layers = num_layers
        self.hidden_size = hidden_size
        self.has_input = has_input
        self.has_output = has_output
        self.device = device

        if has_input:
            self.input = nn.Linear(input_size, embedding_size)
            self.rnn = nn.GRU(
                input_size=embedding_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True,
            )
        else:
            self.rnn = nn.GRU(
                input_size=input_size,
                hidden_size=hidden_size,
                num_layers=num_layers,
                batch_first=True,
            )
        if has_output:
            self.output = nn.Sequential(
                nn.Linear(hidden_size, embedding_size),
                nn.ReLU(),
                nn.Linear(embedding_size, output_size),
            )
        self.relu = nn.ReLU()
        self.hidden = None

        for name, param in self.rnn.named_parameters():
            if "bias" in name:
                nn.init.constant_(param, 0.25)
            elif "weight" in name:
                nn.init.xavier_uniform_(param, gain=nn.init.calculate_gain("sigmoid"))
        for m in self.modules():
            if isinstance(m, nn.Linear):
                m.weight.data = init.xavier_uniform_(
                    m.weight.data, gain=nn.init.calculate_gain("relu")
                )

    def init_hidden(self, batch_size: int) -> Tensor:
        """Initializes the hidden state of GRU.

        Args:
            batch_size: The size of the batch.
        Returns:
            A zero tensor with dimension `(num_layers, batch_size, hidden_size)`.

        """
        return Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)).to(
            self.device
        )

    def forward(self, input_raw: Tensor, pack=False, input_len=None) -> Tensor:
        """Forwards propagates the input tensor.

        Args:
            input_raw: The input tensor.
        Returns:
            Tensor generated by the forward propagation (i.e., the hidden state of the GRU). 

        """
        if self.has_input:
            input = self.input(input_raw)
            input = self.relu(input)
        else:
            input = input_raw
        if pack:
            input = pack_padded_sequence(input, input_len, batch_first=True)
        output_raw, self.hidden = self.rnn(input, self.hidden)
        if pack:
            output_raw = pad_packed_sequence(output_raw, batch_first=True)[0]
        if self.has_output:
            output_raw = self.output(output_raw)
        return output_raw
